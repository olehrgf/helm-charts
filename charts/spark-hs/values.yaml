replicaCount: 1

rbac:
  create: true
  serviceAccount:
    create: true
    annotations: {}
    automountServiceAccountToken: true

image:
  name: apache/spark
  tag: v3.4.1
  pullPolicy: IfNotPresent

spark:
  properties: |
    spark.history.fs.logDirectory=file:/tmp/spark-events

#spark:
#  properties: |
#    spark.history.fs.logDirectory=s3a://<some-bucket>
#    spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
#    spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.InstanceProfileCredentialsProvider
#    spark.history.fs.cleaner.enabled=true
#    spark.history.fs.update.interval=60s
#    spark.history.ui.port=18080
#    spark.history.fs.eventLog.rolling.maxFilesToRetain=5

logger:
  properties: |
    # Set everything to be logged to the console
    log4j.rootCategory=INFO, console
    log4j.appender.console=org.apache.log4j.ConsoleAppender
    log4j.appender.console.target=System.out
    log4j.appender.console.layout=org.apache.log4j.PatternLayout
    log4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n

    # Set the default spark-shell/spark-sql log level to WARN. When running the
    # spark-shell/spark-sql, the log level for these classes is used to overwrite
    # the root logger's log level, so that the user can have different defaults
    # for the shell and regular Spark apps.
    log4j.logger.org.apache.spark.repl.Main=WARN
    log4j.logger.org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver=WARN

    # Settings to quiet third party logs that are too verbose
    log4j.logger.org.sparkproject.jetty=WARN
    log4j.logger.org.sparkproject.jetty.util.component.AbstractLifeCycle=ERROR
    log4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFO
    log4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFO
    log4j.logger.org.apache.parquet=ERROR
    log4j.logger.parquet=ERROR

    # SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive support
    log4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATAL
    log4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR

    # For deploying Spark ThriftServer
    # SPARK-34128ï¼šSuppress undesirable TTransportException warnings involved in THRIFT-4805
    log4j.appender.console.filter.1=org.apache.log4j.varia.StringMatchFilter
    log4j.appender.console.filter.1.StringToMatch=Thrift error occurred during processing of message
    log4j.appender.console.filter.1.AcceptOnMatch=false

    # For Spark history server
    #log4j.logger.org.apache.spark.deploy.history=DEBUG

securityContext: {}
#  fsGroup: 185 # For cache access

env:
  - name: SPARK_HISTORY_OPTS
    value: -Dlog4j.configuration=file:/opt/spark/conf/log4j.properties
  - name: SPARK_NO_DAEMONIZE
    value: "false"
#  - name: SPARK_DAEMON_MEMORY
#    value: 2g

envFrom: []

service:
  type: ClusterIP

resources:
  limits:
    cpu: 1
    memory: 1000Mi
  requests:
    cpu: 400m
    memory: 1000Mi

autoscaling:
  enabled: false
  minReplicas: 1
  maxReplicas: 2
  targetCPUUtilizationPercentage: 80
  targetMemoryUtilizationPercentage: 80

podLabels: {}
#  sidecar.istio.io/inject: "true"

podAnnotations: {}

nodeSelector: {}

tolerations: {}

affinity: {}

extraVolumeMounts: {}

extraVolumes: {}

## For Spark 3.4.x
cache:
  pvc:
    enabled: true
    storageClass: default
    size: 10Gi

